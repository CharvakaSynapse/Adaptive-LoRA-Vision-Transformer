{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "066028ab-ff52-433e-b0c4-0ceb732a3e01",
   "metadata": {},
   "source": [
    "# Attention only FC-LoRA Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874eee3a-876c-4801-ad0e-3d3ef1d9ae40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adaptive Ranks (Attention Only): {'deit.encoder.layer.0.attention.attention.value': 16, 'deit.encoder.layer.9.attention.attention.value': 16, 'deit.encoder.layer.11.attention.attention.value': 16, 'deit.encoder.layer.8.attention.attention.value': 2, 'deit.encoder.layer.4.attention.attention.value': 1, 'deit.encoder.layer.10.attention.attention.value': 1, 'deit.encoder.layer.7.attention.attention.value': 1, 'deit.encoder.layer.3.attention.attention.value': 1, 'deit.encoder.layer.6.attention.attention.value': 1, 'deit.encoder.layer.5.attention.attention.value': 1, 'deit.encoder.layer.2.attention.attention.value': 1, 'deit.encoder.layer.1.attention.attention.value': 1, 'deit.encoder.layer.9.attention.attention.query': 1, 'deit.encoder.layer.8.attention.attention.query': 1, 'deit.encoder.layer.7.attention.attention.query': 1, 'deit.encoder.layer.5.attention.attention.query': 1, 'deit.encoder.layer.3.attention.attention.key': 1, 'deit.encoder.layer.6.attention.attention.query': 1, 'deit.encoder.layer.10.attention.attention.query': 1, 'deit.encoder.layer.3.attention.attention.query': 1, 'deit.encoder.layer.4.attention.attention.key': 1, 'deit.encoder.layer.5.attention.attention.key': 1, 'deit.encoder.layer.9.attention.attention.key': 1, 'deit.encoder.layer.4.attention.attention.query': 1, 'deit.encoder.layer.6.attention.attention.key': 1, 'deit.encoder.layer.8.attention.attention.key': 1, 'deit.encoder.layer.7.attention.attention.key': 1, 'deit.encoder.layer.10.attention.attention.key': 1, 'deit.encoder.layer.11.attention.attention.query': 1, 'deit.encoder.layer.2.attention.attention.key': 1, 'deit.encoder.layer.2.attention.attention.query': 1, 'deit.encoder.layer.11.attention.attention.key': 1, 'deit.encoder.layer.1.attention.attention.query': 1, 'deit.encoder.layer.0.attention.attention.query': 1, 'deit.encoder.layer.1.attention.attention.key': 1, 'deit.encoder.layer.0.attention.attention.key': 1}\n",
      "\n",
      "Final trainable model structure:\n",
      "trainable params: 125,952 || all params: 85,933,834 || trainable%: 0.1466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1250/1250 [02:34<00:00,  8.09it/s, Loss=0.5312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: New best model saved with Val Accuracy: 97.23%\n",
      "Epoch 1: Train Loss: 1.0324, Val Loss: 0.5877, Train Acc: 80.37%, Val Acc: 97.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1250/1250 [02:34<00:00,  8.10it/s, Loss=0.5535]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: New best model saved with Val Accuracy: 97.72%\n",
      "Epoch 2: Train Loss: 0.8545, Val Loss: 0.5654, Train Acc: 85.84%, Val Acc: 97.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1250/1250 [02:33<00:00,  8.14it/s, Loss=0.5181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: New best model saved with Val Accuracy: 97.98%\n",
      "Epoch 3: Train Loss: 0.8144, Val Loss: 0.5561, Train Acc: 88.75%, Val Acc: 97.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1250/1250 [02:30<00:00,  8.28it/s, Loss=0.5924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: New best model saved with Val Accuracy: 98.15%\n",
      "Epoch 4: Train Loss: 0.7908, Val Loss: 0.5536, Train Acc: 88.22%, Val Acc: 98.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1250/1250 [02:33<00:00,  8.16it/s, Loss=0.8673]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Test Loss: 0.5504, Test Accuracy: 98.12%\n",
      "Epoch 5: New best model saved with Val Accuracy: 98.24%\n",
      "Epoch 5: Train Loss: 0.7990, Val Loss: 0.5505, Train Acc: 87.73%, Val Acc: 98.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1250/1250 [02:33<00:00,  8.15it/s, Loss=0.5201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: New best model saved with Val Accuracy: 98.38%\n",
      "Epoch 6: Train Loss: 0.7900, Val Loss: 0.5455, Train Acc: 87.82%, Val Acc: 98.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1250/1250 [02:37<00:00,  7.94it/s, Loss=0.5643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.7829, Val Loss: 0.5464, Train Acc: 88.73%, Val Acc: 98.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1250/1250 [02:34<00:00,  8.07it/s, Loss=0.8801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.7795, Val Loss: 0.5450, Train Acc: 90.20%, Val Acc: 98.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1250/1250 [02:35<00:00,  8.06it/s, Loss=0.5088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.7606, Val Loss: 0.5430, Train Acc: 89.37%, Val Acc: 98.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1250/1250 [02:35<00:00,  8.05it/s, Loss=0.5071]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Test Loss: 0.5432, Test Accuracy: 98.27%\n",
      "Epoch 10: New best model saved with Val Accuracy: 98.48%\n",
      "Epoch 10: Train Loss: 0.7596, Val Loss: 0.5416, Train Acc: 89.64%, Val Acc: 98.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model (Epoch 10): Test Loss: 0.5432, Test Accuracy: 98.27%\n",
      "ECE: 0.0886, Scaled ECE: 0.0045, Scaled Test Accuracy: 98.27%\n",
      "Class-wise Accuracy: Mean 0.98, Std 0.01\n",
      "Total training time: 1902.77 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Subset\n",
    "from transformers import DeiTForImageClassification\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# DATA\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616])\n",
    "])\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "indices = np.arange(len(train_dataset))\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.2 * len(train_dataset))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# MODEL\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "model.classifier.requires_grad_(True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# VALIDATION, ECE, TEMP SCALING\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss, correct, total = 0.0, 0, 0\n",
    "    all_probs, all_labels, all_logits = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), np.concatenate(all_logits)\n",
    "\n",
    "def compute_ece(probs, labels, n_bins=15):\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = (predictions == labels)\n",
    "    ece = 0.0\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    for bin_lower, bin_upper in zip(bin_boundaries[:-1], bin_boundaries[1:]):\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    return ece\n",
    "\n",
    "def find_optimal_temperature(model, loader, device):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            all_logits.append(model(images).logits.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    all_logits = torch.cat(all_logits)\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    temperatures = np.linspace(0.2, 3.0, 100)\n",
    "    best_temp, min_ece = 1.0, float('inf')\n",
    "    for t in temperatures:\n",
    "        scaled_probs = F.softmax(all_logits / t, dim=1).numpy()\n",
    "        ece = compute_ece(scaled_probs, all_labels)\n",
    "        if ece < min_ece:\n",
    "            min_ece, best_temp = ece, t\n",
    "    return best_temp\n",
    "\n",
    "# ADAPTIVE RANK SELECTION \n",
    "fisher_sums = defaultdict(float)\n",
    "grad_sums = defaultdict(float)\n",
    "mean_out = defaultdict(float)\n",
    "sq_mean_out = defaultdict(float)\n",
    "n_batches = 0\n",
    "hook_handles = []\n",
    "target_layers = []\n",
    "\n",
    "def hook_fn(name):\n",
    "    def hook(module, input, output):\n",
    "        output_tensor = output[0] if isinstance(output, tuple) else output\n",
    "        batch_out_flat = output_tensor.detach().view(-1, output_tensor.shape[-1])\n",
    "        mean_out[name] += batch_out_flat.mean(dim=0)\n",
    "        sq_mean_out[name] += (batch_out_flat ** 2).mean(dim=0)\n",
    "    return hook\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear) and any(t in name for t in [\"query\", \"key\", \"value\"]):\n",
    "        target_layers.append(name)\n",
    "        handle = module.register_forward_hook(hook_fn(name))\n",
    "        hook_handles.append(handle)\n",
    "\n",
    "temp_optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "model.train()\n",
    "for i, (images, labels) in enumerate(tqdm(train_loader, desc=\"Compute Importance\", leave=False), 1):\n",
    "    if i > 100: break\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    temp_optimizer.zero_grad()\n",
    "    loss = criterion(model(images).logits, labels)\n",
    "    loss.backward()\n",
    "    n_batches += 1\n",
    "    for name, module in model.named_modules():\n",
    "        if name in target_layers:\n",
    "            for p_name, p in [(\"weight\", module.weight), (\"bias\", getattr(module, 'bias', None))]:\n",
    "                if p is not None and p.grad is not None:\n",
    "                    fisher_sums[f\"{name}.{p_name}\"] += (p.grad ** 2).mean().item()\n",
    "                    grad_sums[f\"{name}.{p_name}\"] += p.grad.abs().mean().item()\n",
    "for handle in hook_handles: handle.remove()\n",
    "\n",
    "cov_trace = {name: ((sq_mean_out[name] / n_batches) - ((mean_out[name] / n_batches) ** 2)).sum().item() for name in mean_out}\n",
    "combined_importance = {}\n",
    "fisher_min, fisher_max = min(fisher_sums.values()), max(fisher_sums.values())\n",
    "grad_min, grad_max = min(grad_sums.values()), max(grad_sums.values())\n",
    "cov_min, cov_max = (min(cov_trace.values()), max(cov_trace.values())) if cov_trace else (0, 0)\n",
    "for name in target_layers:\n",
    "    fisher_score = sum(fisher_sums.get(p, 0) for p in [f\"{name}.weight\", f\"{name}.bias\"])\n",
    "    grad_score = sum(grad_sums.get(p, 0) for p in [f\"{name}.weight\", f\"{name}.bias\"])\n",
    "    cov_score = cov_trace.get(name, 0)\n",
    "    fisher_z = (fisher_score - fisher_min) / (fisher_max - fisher_min + 1e-6)\n",
    "    grad_z = (grad_score - grad_min) / (grad_max - grad_min + 1e-6)\n",
    "    cov_z = (cov_score - cov_min) / (cov_max - cov_min + 1e-6) if cov_trace else 0\n",
    "    score = 0.6 * fisher_z + 0.2 * grad_z + 0.2 * cov_z\n",
    "    combined_importance[name] = score\n",
    "\n",
    "r_max, r_min, r_total = 16, 1, 50\n",
    "adaptive_ranks = {}\n",
    "sorted_layers = sorted(combined_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "remaining_budget = r_total\n",
    "for name, score in sorted_layers:\n",
    "    if remaining_budget <= 0:\n",
    "        adaptive_ranks[name] = r_min\n",
    "    else:\n",
    "        rank = min(max(r_min, int(score * r_total)), r_max, remaining_budget)\n",
    "        adaptive_ranks[name] = rank\n",
    "        remaining_budget -= rank\n",
    "if remaining_budget > 0:\n",
    "    for name in [n for n, s in sorted_layers][:remaining_budget]:\n",
    "        adaptive_ranks[name] = min(adaptive_ranks.get(name, 0) + 1, r_max)\n",
    "print(\"\\nAdaptive Ranks (Attention Only):\", adaptive_ranks)\n",
    "\n",
    "# PEFT INJECTION + MANUAL RANK ADJUSTMENT \n",
    "lora_config = LoraConfig(\n",
    "    r=r_max,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=list(adaptive_ranks.keys()),\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.to(device)\n",
    "\n",
    "# Manually set per-layer rank\n",
    "for name, rank in adaptive_ranks.items():\n",
    "    module = model.get_submodule(name)\n",
    "    if hasattr(module, 'r'):\n",
    "        if isinstance(module.r, dict) and 'default' in module.r:\n",
    "            module.r['default'] = rank\n",
    "        else:\n",
    "            module.r = rank\n",
    "        in_features = module.in_features\n",
    "        out_features = module.out_features\n",
    "        module.lora_A['default'].weight.data = torch.randn(rank, in_features).to(device) * 0.02\n",
    "        module.lora_B['default'].weight.data = torch.zeros(out_features, rank).to(device)\n",
    "\n",
    "print(\"\\nFinal trainable model structure:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# TRAINING \n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora_' in name or 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    W, H = images.size(3), images.size(2)\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)\n",
    "    cx, cy = np.random.randint(W), np.random.randint(H)\n",
    "    bbx1, bby1 = np.clip(cx - cut_w // 2, 0, W), np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2, bby2 = np.clip(cx + cut_w // 2, 0, W), np.clip(cy + cut_h // 2, 0, H)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(2) * images.size(3)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n",
    "best_val_accuracy, best_epoch = 0.0, 1\n",
    "best_model_path = \"deit_cifar10_hybrid_LoRA_PEFT.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    train_loss, train_correct, train_total = 0.0, 0, 0\n",
    "    if epoch < 5:\n",
    "        lr = 5e-4 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\", leave=True)\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\"})\n",
    "    progress_bar.close()\n",
    "    scheduler.step()\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    val_loss, val_accuracy, val_probs, val_labels, _ = validate(model, val_loader, criterion, device)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, *_ = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}: Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"Epoch {epoch+1}: New best model saved with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "# EVAL/LOGGING \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig('deit_cifar10_hybrid_LoRA_PEFT_fastlr.png')\n",
    "plt.close()\n",
    "\n",
    "# FINAL EVAL\n",
    "base_model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "base_model.classifier = torch.nn.Linear(base_model.classifier.in_features, 10)\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.to(device)\n",
    "for name, rank in adaptive_ranks.items():\n",
    "    module = model.get_submodule(name)\n",
    "    if hasattr(module, 'r'):\n",
    "        if isinstance(module.r, dict) and 'default' in module.r:\n",
    "            module.r['default'] = rank\n",
    "        else:\n",
    "            module.r = rank\n",
    "        in_features = module.in_features\n",
    "        out_features = module.out_features\n",
    "        module.lora_A['default'].weight.data = torch.randn(rank, in_features).to(device) * 0.02\n",
    "        module.lora_B['default'].weight.data = torch.zeros(out_features, rank).to(device)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "optimal_temp = find_optimal_temperature(model, val_loader, device)\n",
    "scaled_test_probs = F.softmax(torch.tensor(test_logits) / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}): Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "# torch.save(model.state_dict(), \"deit_cifar10_hybrid_LoRA_PEFT_final.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38301b9-95b5-4ad8-b929-65418ed65f29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
