# FC-LoRA âš¡ Attention-Only Fisher-Grad-Covariance LoRA for DeiT

> **Train 0.23â€¯% of DeiT-Base, keep â‰¥99â€¯% of its accuracy.**

---

## âœ¨ Highlights

- **Attention-Only:** Adapters added *only* to Q/K/V projectionsâ€”no MLP or norm mods.
- **Fisher Ã— Grad Ã— Covariance:** Layer importance = 0.6Â·Fisher + 0.2Â·Gradient + 0.2Â·Covariance.
- **Rank Budgeting:** Water-fill 100 total ranks, â‰¤16 per layer.
- **Tiny Foot-print:** 198k / 86M trainable â†’ **0.23â€¯%**.
- **Fast:** 15 epochs â‡’ ~48 min on one RTX 3090.
- **Well-Calibrated:** Scaled ECE â‰ˆ 0.015 (CIFAR-100).

---

## ğŸ“Š Results

### CIFAR-100 â€“ 15 epochs

| Metric               | FC-LoRA                | Full Finetune        |
|----------------------|------------------------|----------------------|
| **Trainable params** | **198,144 (0.23â€¯%)**   | 86,075,236 (100â€¯%)   |
| **Test Accuracy**    | **89.19â€¯%**            | 90.18â€¯%              |
| **Test Loss**        | 1.1284                 | 1.1345               |
| **ECE / Scaled ECE** | 0.0920 / **0.0146**    | 0.0560 / 0.0266      |
| **Class Acc ÂµÂ±Ïƒ**    | 0.89â€¯Â±â€¯0.08            | 0.90â€¯Â±â€¯0.07          |
| **Wall-clock**       | **2,862s (~48 min)**   | 6,821s (~1h 54m)     |

### CIFAR-10 â€“ 10 epochs

| Metric               | FC-LoRA                | Full Finetune      |
|----------------------|------------------------|--------------------|
| **Trainable params** | **125,952 (0.15â€¯%)**   | 85,933,834 (100â€¯%) |
| **Test Accuracy**    | **98.27â€¯%**            | 98.51â€¯%            |
| **ECE / Scaled ECE** | 0.0886 / **0.0045**    | 0.0840 / 0.0051    |
| **Wall-clock**       | **1,903s (~32 min)**   | 2,279s (~38 min)   |

---
## ğŸ† Results Visualization

<img src="./cifar100_acc_vs_time_refined.PNG" alt="CIFAR-100 Accuracy vs Training Time" width="600"/>

**Figure:** *Accuracy vs. Training Time for Classifier-Only, FC-LoRA (ours), and Full Fine-Tuning on CIFAR-100 (DeiT-Base, L40S GPU).*

- FC-LoRA achieves >98% of full fine-tuning accuracy in less than half the compute.
- All results are from our reproducible runs (see table above).

---
## âš¡ How It Works

- **Attention-Only LoRA:**  
  LoRA adapters are injected *exclusively* into the Q, K, and V projection layers of each transformer block. No adapters touch the MLP or normalization layers.

- **Fisher Ã— Grad Ã— Covariance Importance:**  
  For each eligible attention layer, a short pass over training data computes a composite â€œimportanceâ€ score:
    - 0.6 Ã— Fisher information (diagonal, from squared gradients)
    - 0.2 Ã— Mean gradient magnitude
    - 0.2 Ã— Activation covariance trace  
  All three scores are z-normalized and combined to guide rank allocation.

- **Rank Budgeting:**  
  Given a total budget (e.g., 100 ranks), ranks are distributed to layers in proportion to their normalized importance, with an upper limit (e.g., 16) per layer (â€œwater-fillingâ€).

- **Tiny Parameter Footprint:**  
  Only the LoRA adapters and classifier are trainable. The rest of the DeiT model stays frozen.

- **Calibration by Temperature Scaling:**  
  After training, a scalar temperature is tuned on the validation set to further reduce ECE.

---


