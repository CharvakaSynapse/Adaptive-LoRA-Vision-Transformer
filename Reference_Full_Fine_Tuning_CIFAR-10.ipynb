{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de24f726-7c99-42ab-a696-08d6b73eba01",
   "metadata": {},
   "source": [
    "# DEIT full fine tune CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ce50be-361d-43a6-b9bc-2a32591d7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program starts...\n",
      "Running Classifier-Only Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:01<00:00, 91460144.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters before training:\n",
      "Total trainable params: 7690\n",
      "New best model saved at epoch 1 with Val Accuracy: 94.13%\n",
      "Epoch 1, Train Loss: 1.1643, Val Loss: 0.6939, Train Accuracy: 76.69%, Val Accuracy: 94.13%, Time: 100.80s, LR: 0.000499\n",
      "New best model saved at epoch 2 with Val Accuracy: 95.22%\n",
      "Epoch 2, Train Loss: 0.9845, Val Loss: 0.6637, Train Accuracy: 81.86%, Val Accuracy: 95.22%, Time: 98.87s, LR: 0.000495\n",
      "New best model saved at epoch 3 with Val Accuracy: 95.46%\n",
      "Epoch 3, Train Loss: 0.9465, Val Loss: 0.6524, Train Accuracy: 84.70%, Val Accuracy: 95.46%, Time: 99.91s, LR: 0.000488\n",
      "New best model saved at epoch 4 with Val Accuracy: 95.69%\n",
      "Epoch 4, Train Loss: 0.9258, Val Loss: 0.6497, Train Accuracy: 84.20%, Val Accuracy: 95.69%, Time: 99.26s, LR: 0.000478\n",
      "Epoch 5 - Test Loss: 0.6576, Test Accuracy: 95.24%\n",
      "Epoch 5, Train Loss: 0.9424, Val Loss: 0.6512, Train Accuracy: 83.56%, Val Accuracy: 95.61%, Time: 99.90s, LR: 0.000467\n",
      "Epoch 6, Train Loss: 0.9433, Val Loss: 0.6475, Train Accuracy: 83.50%, Val Accuracy: 95.67%, Time: 100.16s, LR: 0.000452\n",
      "New best model saved at epoch 7 with Val Accuracy: 95.88%\n",
      "Epoch 7, Train Loss: 0.9369, Val Loss: 0.6455, Train Accuracy: 84.23%, Val Accuracy: 95.88%, Time: 100.85s, LR: 0.000436\n",
      "Epoch 8, Train Loss: 0.9404, Val Loss: 0.6472, Train Accuracy: 84.91%, Val Accuracy: 95.80%, Time: 99.94s, LR: 0.000417\n",
      "Epoch 9, Train Loss: 0.9272, Val Loss: 0.6463, Train Accuracy: 84.59%, Val Accuracy: 95.74%, Time: 99.13s, LR: 0.000397\n",
      "Epoch 10 - Test Loss: 0.6525, Test Accuracy: 95.26%\n",
      "New best model saved at epoch 10 with Val Accuracy: 95.92%\n",
      "Epoch 10, Train Loss: 0.9252, Val Loss: 0.6467, Train Accuracy: 84.80%, Val Accuracy: 95.92%, Time: 99.62s, LR: 0.000375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model (Epoch 10) - Test Loss: 0.6525, Test Accuracy: 95.26%\n",
      "ECE: 0.1274, Scaled ECE: 0.0228, Scaled Test Accuracy: 95.26%\n",
      "Class-wise Accuracy: Mean 0.95, Std 0.02\n",
      "Total training time: 1088.29 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from transformers import DeiTForImageClassification\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "start = time.time()\n",
    "print('Program starts...')\n",
    "print(\"Running Classifier-Only Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "assert len(set(train_indices) & set(val_indices)) == 0, \"Train-validation overlap detected\"\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DeiT and modify classifier\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "\n",
    "# Freeze all parameters except classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameters before training:\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable params: {trainable_params}\")\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), torch.cat(all_logits)\n",
    "\n",
    "# Compute ECE\n",
    "def compute_ece(probs, labels, n_bins=10):\n",
    "    probs = np.clip(probs, 1e-5, 1-1e-5)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += prop_in_bin * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "    return ece\n",
    "\n",
    "# CutMix function\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Setup training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = \"deit_cifar10_classifier_only_cutmix_best_seed78.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        lr = 5e-4 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    scheduler.step(epoch + 1)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    val_loss, val_accuracy, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {best_epoch} with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('classifier_only_cutmix_metrics_cifar10_seed78.png')\n",
    "plt.close()\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate test set with calibration\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "\n",
    "# Temperature scaling\n",
    "def find_optimal_temperature(val_logits, val_labels):\n",
    "    def ece_with_temp(temp):\n",
    "        scaled_probs = F.softmax(val_logits / temp, dim=1).numpy()\n",
    "        scaled_probs = np.clip(scaled_probs, 1e-5, 1-1e-5)\n",
    "        return compute_ece(scaled_probs, val_labels)\n",
    "    \n",
    "    temps = np.linspace(0.1, 5.0, 20)\n",
    "    eces = [ece_with_temp(t) for t in temps]\n",
    "    return temps[np.argmin(eces)]\n",
    "\n",
    "_, _, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "optimal_temp = find_optimal_temperature(val_logits, val_labels)\n",
    "scaled_test_probs = F.softmax(test_logits / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}) - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"deit_cifar10_classifier_only_cutmix_final_seed78.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f5c6cc-7f23-49a2-9ced-4386c3bdf0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837d597-49b6-4d2d-a571-36a44ae8823c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf0538a-3c61-4c6f-98a2-29a2b87d6b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program starts...\n",
      "Running Baseline AR-LoRA with CutMix on CIFAR-10 (15 Epochs, No SWA, No Freezing)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total allocated ranks: 100\n",
      "Rank allocation iterations: 10\n",
      "Trainable parameters before training:\n",
      "Total trainable params: 161290\n",
      "LoRA layers: ['deit.encoder.layer.0.attention.attention.query.lora.lora_A', 'deit.encoder.layer.0.attention.attention.query.lora.lora_B', 'deit.encoder.layer.0.attention.attention.key.lora.lora_A', 'deit.encoder.layer.0.attention.attention.key.lora.lora_B', 'deit.encoder.layer.0.attention.attention.value.lora.lora_A', 'deit.encoder.layer.0.attention.attention.value.lora.lora_B', 'deit.encoder.layer.1.attention.attention.query.lora.lora_A', 'deit.encoder.layer.1.attention.attention.query.lora.lora_B', 'deit.encoder.layer.1.attention.attention.key.lora.lora_A', 'deit.encoder.layer.1.attention.attention.key.lora.lora_B', 'deit.encoder.layer.1.attention.attention.value.lora.lora_A', 'deit.encoder.layer.1.attention.attention.value.lora.lora_B', 'deit.encoder.layer.2.attention.attention.query.lora.lora_A', 'deit.encoder.layer.2.attention.attention.query.lora.lora_B', 'deit.encoder.layer.2.attention.attention.key.lora.lora_A', 'deit.encoder.layer.2.attention.attention.key.lora.lora_B', 'deit.encoder.layer.2.attention.attention.value.lora.lora_A', 'deit.encoder.layer.2.attention.attention.value.lora.lora_B', 'deit.encoder.layer.3.attention.attention.query.lora.lora_A', 'deit.encoder.layer.3.attention.attention.query.lora.lora_B', 'deit.encoder.layer.3.attention.attention.key.lora.lora_A', 'deit.encoder.layer.3.attention.attention.key.lora.lora_B', 'deit.encoder.layer.3.attention.attention.value.lora.lora_A', 'deit.encoder.layer.3.attention.attention.value.lora.lora_B', 'deit.encoder.layer.4.attention.attention.query.lora.lora_A', 'deit.encoder.layer.4.attention.attention.query.lora.lora_B', 'deit.encoder.layer.4.attention.attention.key.lora.lora_A', 'deit.encoder.layer.4.attention.attention.key.lora.lora_B', 'deit.encoder.layer.4.attention.attention.value.lora.lora_A', 'deit.encoder.layer.4.attention.attention.value.lora.lora_B', 'deit.encoder.layer.5.attention.attention.query.lora.lora_A', 'deit.encoder.layer.5.attention.attention.query.lora.lora_B', 'deit.encoder.layer.5.attention.attention.key.lora.lora_A', 'deit.encoder.layer.5.attention.attention.key.lora.lora_B', 'deit.encoder.layer.5.attention.attention.value.lora.lora_A', 'deit.encoder.layer.5.attention.attention.value.lora.lora_B', 'deit.encoder.layer.6.attention.attention.query.lora.lora_A', 'deit.encoder.layer.6.attention.attention.query.lora.lora_B', 'deit.encoder.layer.6.attention.attention.key.lora.lora_A', 'deit.encoder.layer.6.attention.attention.key.lora.lora_B', 'deit.encoder.layer.6.attention.attention.value.lora.lora_A', 'deit.encoder.layer.6.attention.attention.value.lora.lora_B', 'deit.encoder.layer.7.attention.attention.query.lora.lora_A', 'deit.encoder.layer.7.attention.attention.query.lora.lora_B', 'deit.encoder.layer.7.attention.attention.key.lora.lora_A', 'deit.encoder.layer.7.attention.attention.key.lora.lora_B', 'deit.encoder.layer.7.attention.attention.value.lora.lora_A', 'deit.encoder.layer.7.attention.attention.value.lora.lora_B', 'deit.encoder.layer.8.attention.attention.query.lora.lora_A', 'deit.encoder.layer.8.attention.attention.query.lora.lora_B', 'deit.encoder.layer.8.attention.attention.key.lora.lora_A', 'deit.encoder.layer.8.attention.attention.key.lora.lora_B', 'deit.encoder.layer.8.attention.attention.value.lora.lora_A', 'deit.encoder.layer.8.attention.attention.value.lora.lora_B', 'deit.encoder.layer.9.attention.attention.query.lora.lora_A', 'deit.encoder.layer.9.attention.attention.query.lora.lora_B', 'deit.encoder.layer.9.attention.attention.key.lora.lora_A', 'deit.encoder.layer.9.attention.attention.key.lora.lora_B', 'deit.encoder.layer.9.attention.attention.value.lora.lora_A', 'deit.encoder.layer.9.attention.attention.value.lora.lora_B', 'deit.encoder.layer.10.attention.attention.query.lora.lora_A', 'deit.encoder.layer.10.attention.attention.query.lora.lora_B', 'deit.encoder.layer.10.attention.attention.key.lora.lora_A', 'deit.encoder.layer.10.attention.attention.key.lora.lora_B', 'deit.encoder.layer.10.attention.attention.value.lora.lora_A', 'deit.encoder.layer.10.attention.attention.value.lora.lora_B', 'deit.encoder.layer.11.attention.attention.query.lora.lora_A', 'deit.encoder.layer.11.attention.attention.query.lora.lora_B', 'deit.encoder.layer.11.attention.attention.key.lora.lora_A', 'deit.encoder.layer.11.attention.attention.key.lora.lora_B', 'deit.encoder.layer.11.attention.attention.value.lora.lora_A', 'deit.encoder.layer.11.attention.attention.value.lora.lora_B']\n",
      "New best model saved at epoch 1 with Val Accuracy: 97.35%\n",
      "Epoch 1, Train Loss: 0.9912, Val Loss: 0.5768, Train Accuracy: 81.56%, Val Accuracy: 97.35%, Time: 171.01s, LR: 0.000499\n",
      "LoRA weight norm: 47.6471\n",
      "New best model saved at epoch 2 with Val Accuracy: 97.78%\n",
      "Epoch 2, Train Loss: 0.8460, Val Loss: 0.5594, Train Accuracy: 85.81%, Val Accuracy: 97.78%, Time: 170.12s, LR: 0.000495\n",
      "LoRA weight norm: 58.2108\n",
      "New best model saved at epoch 3 with Val Accuracy: 97.93%\n",
      "Epoch 3, Train Loss: 0.8097, Val Loss: 0.5541, Train Accuracy: 88.95%, Val Accuracy: 97.93%, Time: 169.74s, LR: 0.000488\n",
      "LoRA weight norm: 71.5713\n",
      "New best model saved at epoch 4 with Val Accuracy: 97.99%\n",
      "Epoch 4, Train Loss: 0.7843, Val Loss: 0.5519, Train Accuracy: 88.58%, Val Accuracy: 97.99%, Time: 169.55s, LR: 0.000478\n",
      "LoRA weight norm: 85.0841\n",
      "Epoch 5 - Test Loss: 0.5542, Test Accuracy: 97.91%\n",
      "Epoch 5, Train Loss: 0.7944, Val Loss: 0.5543, Train Accuracy: 87.70%, Val Accuracy: 97.91%, Time: 168.97s, LR: 0.000467\n",
      "LoRA weight norm: 98.5933\n",
      "New best model saved at epoch 6 with Val Accuracy: 98.01%\n",
      "Epoch 6, Train Loss: 0.7873, Val Loss: 0.5510, Train Accuracy: 88.02%, Val Accuracy: 98.01%, Time: 168.48s, LR: 0.000452\n",
      "LoRA weight norm: 106.7647\n",
      "Epoch 7, Train Loss: 0.7786, Val Loss: 0.5563, Train Accuracy: 88.94%, Val Accuracy: 97.90%, Time: 169.80s, LR: 0.000436\n",
      "LoRA weight norm: 113.0307\n",
      "New best model saved at epoch 8 with Val Accuracy: 98.12%\n",
      "Epoch 8, Train Loss: 0.7791, Val Loss: 0.5478, Train Accuracy: 90.21%, Val Accuracy: 98.12%, Time: 169.20s, LR: 0.000417\n",
      "LoRA weight norm: 118.4799\n",
      "Epoch 9, Train Loss: 0.7617, Val Loss: 0.5487, Train Accuracy: 89.50%, Val Accuracy: 98.03%, Time: 168.17s, LR: 0.000397\n",
      "LoRA weight norm: 122.5871\n",
      "Epoch 10 - Test Loss: 0.5527, Test Accuracy: 97.94%\n",
      "Epoch 10, Train Loss: 0.7575, Val Loss: 0.5502, Train Accuracy: 89.67%, Val Accuracy: 98.07%, Time: 169.59s, LR: 0.000375\n",
      "LoRA weight norm: 125.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model (Epoch 8) - Test Loss: 0.5505, Test Accuracy: 98.08%\n",
      "ECE: 0.0834, Scaled ECE: 0.0060, Scaled Test Accuracy: 98.08%\n",
      "Class-wise Accuracy: Mean 0.98, Std 0.01\n",
      "Total training time: 1786.07 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from transformers import DeiTForImageClassification\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "start = time.time()\n",
    "print('Program starts...')\n",
    "print(\"Running Baseline AR-LoRA with CutMix on CIFAR-10 (15 Epochs, No SWA, No Freezing)\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "assert len(set(train_indices) & set(val_indices)) == 0, \"Train-validation overlap detected\"\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DeiT\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "model.classifier.weight.requires_grad = True\n",
    "model.classifier.bias.requires_grad = True\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), torch.cat(all_logits)\n",
    "\n",
    "# Compute ECE\n",
    "def compute_ece(probs, labels, n_bins=10):\n",
    "    probs = np.clip(probs, 1e-5, 1-1e-5)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += prop_in_bin * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "    return ece\n",
    "\n",
    "# CutMix function\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Compute importance scores\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "importance_scores = defaultdict(list)\n",
    "output_variances = defaultdict(list)\n",
    "\n",
    "def hook_fn(name):\n",
    "    def hook(module, input, output):\n",
    "        output_variances[name].append(output.var(dim=0).mean().item())\n",
    "    return hook\n",
    "\n",
    "hook_handles = []\n",
    "target_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear) and any(t in name for t in [\"query\", \"key\", \"value\", \"fc1\", \"fc2\"]):\n",
    "        target_layers.append(name)\n",
    "        handle = module.register_forward_hook(hook_fn(name))\n",
    "        hook_handles.append(handle)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images).logits\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear) and name in target_layers:\n",
    "            if module.weight.grad is not None:\n",
    "                grad_mean = module.weight.grad.abs().mean().item()\n",
    "                if not torch.isnan(torch.tensor(grad_mean)):\n",
    "                    importance_scores[name].append(grad_mean)\n",
    "            else:\n",
    "                importance_scores[name].append(1e-8)\n",
    "\n",
    "for handle in hook_handles:\n",
    "    handle.remove()\n",
    "\n",
    "# Average and normalize importance scores\n",
    "combined_importance = {}\n",
    "for name in importance_scores:\n",
    "    grad_vals = np.array(importance_scores[name])\n",
    "    var_vals = np.array(output_variances[name])\n",
    "    if len(grad_vals) == 0 or len(var_vals) == 0:\n",
    "        print(f\"Warning: No valid scores for {name}, skipping\")\n",
    "        continue\n",
    "    grad_vals = np.clip(grad_vals, 1e-8, 1e2)\n",
    "    var_vals = np.clip(var_vals, 1e-8, 1e2)\n",
    "    z_grad = (grad_vals - grad_vals.mean()) / (grad_vals.std() + 1e-6)\n",
    "    z_var = (var_vals - var_vals.mean()) / (var_vals.std() + 1e-6)\n",
    "    score = 0.5 * z_grad.mean() + 0.5 * z_var.mean()\n",
    "    if not np.isnan(score):\n",
    "        combined_importance[name] = score\n",
    "\n",
    "if not combined_importance:\n",
    "    raise ValueError(\"No valid importance scores computed.\")\n",
    "\n",
    "total_importance = max(sum(abs(v) for v in combined_importance.values()), 1e-8)\n",
    "importance_norm = {k: v / total_importance for k, v in combined_importance.items()}\n",
    "\n",
    "# Proportional rank allocation with budget enforcement\n",
    "r_max, r_min, r_total = 16, 1, 100\n",
    "adaptive_ranks = {}\n",
    "total_score = sum(importance_norm.values())\n",
    "for name, score in importance_norm.items():\n",
    "    rank = max(r_min, min(r_max, int(score / total_score * r_total)))\n",
    "    adaptive_ranks[name] = rank\n",
    "\n",
    "allocated = sum(adaptive_ranks.values())\n",
    "iteration_count = 0\n",
    "while allocated != r_total:\n",
    "    iteration_count += 1\n",
    "    if allocated > r_total:\n",
    "        for name in sorted(adaptive_ranks, key=lambda x: importance_norm.get(x, 0), reverse=True):\n",
    "            if adaptive_ranks[name] > r_min:\n",
    "                adaptive_ranks[name] -= 1\n",
    "                allocated -= 1\n",
    "                if allocated == r_total:\n",
    "                    break\n",
    "    else:\n",
    "        for name in sorted(adaptive_ranks, key=lambda x: importance_norm.get(x, 0)):\n",
    "            if adaptive_ranks[name] < r_max:\n",
    "                adaptive_ranks[name] += 1\n",
    "                allocated += 1\n",
    "                if allocated == r_total:\n",
    "                    break\n",
    "print(f\"Total allocated ranks: {sum(adaptive_ranks.values())}\")\n",
    "print(f\"Rank allocation iterations: {iteration_count}\")\n",
    "\n",
    "# Custom LoRA Layer\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, alpha=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.scale = alpha / rank if rank > 0 else 0\n",
    "        self.lora_A = torch.nn.Parameter(torch.randn(rank, in_features) * 0.02)\n",
    "        self.lora_B = torch.nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = F.linear(self.dropout(x), self.lora_A)\n",
    "        r = F.linear(r, self.lora_B) * self.scale\n",
    "        return r\n",
    "\n",
    "def compute_correlation(model, val_loader, criterion, device, importance_norm):\n",
    "    model.eval()\n",
    "    baseline_loss, _, _, _, _ = validate(model, val_loader, criterion, device)\n",
    "    loss_deltas = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoRAWrappedLinear) and name in importance_norm:\n",
    "            lora_layer = module.lora\n",
    "            original_A = lora_layer.lora_A.data.clone()\n",
    "            original_B = lora_layer.lora_B.data.clone()\n",
    "            lora_layer.lora_A.data.zero_()\n",
    "            lora_layer.lora_B.data.zero_()\n",
    "            ablated_loss, _, _, _, _ = validate(model, val_loader, criterion, device)\n",
    "            loss_deltas[name] = ablated_loss - baseline_loss\n",
    "            lora_layer.lora_A.data = original_A\n",
    "            lora_layer.lora_B.data = original_B\n",
    "    \n",
    "    common_keys = set(loss_deltas.keys()) & set(importance_norm.keys())\n",
    "    if len(common_keys) < 2:\n",
    "        print(\"Warning: Insufficient layers for correlation, returning 0.0\")\n",
    "        return 0.0\n",
    "    \n",
    "    importance_vals = [importance_norm[n] for n in common_keys]\n",
    "    delta_vals = [loss_deltas[n] for n in common_keys]\n",
    "    \n",
    "    if np.any(np.isnan(importance_vals)) or np.any(np.isnan(delta_vals)):\n",
    "        print(\"Warning: Invalid data for correlation, returning 0.0\")\n",
    "        return 0.0\n",
    "    \n",
    "    correlation, _ = spearmanr(importance_vals, delta_vals)\n",
    "    return correlation\n",
    "\n",
    "# Define LoRAWrappedLinear globally\n",
    "class LoRAWrappedLinear(torch.nn.Module):\n",
    "    def __init__(self, linear, lora):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = lora\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "# Inject LoRA into model\n",
    "def inject_lora(model, adaptive_ranks, lora_alpha=16, lora_dropout=0.1):\n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, torch.nn.Linear) and name in adaptive_ranks:\n",
    "            rank = adaptive_ranks[name]\n",
    "            if rank == 0:\n",
    "                continue\n",
    "            module.weight.requires_grad = False\n",
    "            if module.bias is not None:\n",
    "                module.bias.requires_grad = False\n",
    "            lora_layer = LoRALayer(\n",
    "                in_features=module.in_features,\n",
    "                out_features=module.out_features,\n",
    "                rank=rank,\n",
    "                alpha=lora_alpha,\n",
    "                dropout=lora_dropout\n",
    "            ).to(module.weight.device)\n",
    "            wrapped_layer = LoRAWrappedLinear(module, lora_layer)\n",
    "            parent_name = name.rsplit('.', 1)[0]\n",
    "            child_name = name.rsplit('.', 1)[1]\n",
    "            parent = model\n",
    "            for part in parent_name.split('.'):\n",
    "                parent = getattr(parent, part)\n",
    "            setattr(parent, child_name, wrapped_layer)\n",
    "\n",
    "# Apply custom LoRA\n",
    "model.to(device)\n",
    "inject_lora(model, adaptive_ranks, lora_alpha=16, lora_dropout=0.1)\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameters before training:\")\n",
    "trainable_params = 0\n",
    "lora_layers = []\n",
    "for name, param in model.named_parameters():\n",
    "    if not ('lora_A' in name or 'lora_B' in name or 'classifier' in name):\n",
    "        param.requires_grad = False\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        if 'lora_' in name:\n",
    "            lora_layers.append(name)\n",
    "print(f\"Total trainable params: {trainable_params}\")\n",
    "print(f\"LoRA layers: {lora_layers}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "\n",
    "# Correlation analysis\n",
    "# correlation = compute_correlation(model, val_loader, criterion, device, importance_norm)\n",
    "# print(f\"Spearman's correlation: {correlation:.4f}\")\n",
    "\n",
    "# Fine-tune\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=1e-6)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = \"deit_cifar10_ar_lora_baseline_cutmix_best_seed78.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        lr = 5e-4 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    scheduler.step(epoch + 1)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    val_loss, val_accuracy, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {best_epoch} with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    lora_norm = sum(torch.norm(param).item() for name, param in model.named_parameters() if 'lora_' in name)\n",
    "    print(f\"LoRA weight norm: {lora_norm:.4f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ar_lora_baseline_cutmix_metrics_cifar10_seed78.png')\n",
    "plt.close()\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "inject_lora(model, adaptive_ranks, lora_alpha=16, lora_dropout=0.1)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate test set with calibration\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "\n",
    "# Temperature scaling\n",
    "def find_optimal_temperature(val_logits, val_labels):\n",
    "    def ece_with_temp(temp):\n",
    "        scaled_probs = F.softmax(val_logits / temp, dim=1).numpy()\n",
    "        scaled_probs = np.clip(scaled_probs, 1e-5, 1-1e-5)\n",
    "        return compute_ece(scaled_probs, val_labels)\n",
    "    \n",
    "    temps = np.linspace(0.1, 5.0, 20)\n",
    "    eces = [ece_with_temp(t) for t in temps]\n",
    "    return temps[np.argmin(eces)]\n",
    "\n",
    "_, _, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "optimal_temp = find_optimal_temperature(val_logits, val_labels)\n",
    "scaled_test_probs = F.softmax(test_logits / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}) - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"deit_cifar10_ar_lora_baseline_cutmix_final_seed78.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb42b70-21a1-400e-977e-31aa2b593cc1",
   "metadata": {},
   "source": [
    "# AR-LoRA 1e-5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d99ad-b287-40fc-8ab8-2f678ada4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from transformers import DeiTForImageClassification\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import spearmanr\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "start = time.time()\n",
    "print('Program starts...')\n",
    "print(\"Running Baseline AR-LoRA with CutMix on CIFAR-10 (15 Epochs, No SWA, No Freezing)\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "assert len(set(train_indices) & set(val_indices)) == 0, \"Train-validation overlap detected\"\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DeiT\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "model.classifier.weight.requires_grad = True\n",
    "model.classifier.bias.requires_grad = True\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), torch.cat(all_logits)\n",
    "\n",
    "# Compute ECE\n",
    "def compute_ece(probs, labels, n_bins=10):\n",
    "    probs = np.clip(probs, 1e-5, 1-1e-5)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += prop_in_bin * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "    return ece\n",
    "\n",
    "# CutMix function\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Compute importance scores\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "importance_scores = defaultdict(list)\n",
    "output_variances = defaultdict(list)\n",
    "\n",
    "def hook_fn(name):\n",
    "    def hook(module, input, output):\n",
    "        output_variances[name].append(output.var(dim=0).mean().item())\n",
    "    return hook\n",
    "\n",
    "hook_handles = []\n",
    "target_layers = []\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, torch.nn.Linear) and any(t in name for t in [\"query\", \"key\", \"value\", \"fc1\", \"fc2\"]):\n",
    "        target_layers.append(name)\n",
    "        handle = module.register_forward_hook(hook_fn(name))\n",
    "        hook_handles.append(handle)\n",
    "\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    outputs = model(images).logits\n",
    "    loss = criterion(outputs, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear) and name in target_layers:\n",
    "            if module.weight.grad is not None:\n",
    "                grad_mean = module.weight.grad.abs().mean().item()\n",
    "                if not torch.isnan(torch.tensor(grad_mean)):\n",
    "                    importance_scores[name].append(grad_mean)\n",
    "            else:\n",
    "                importance_scores[name].append(1e-8)\n",
    "\n",
    "for handle in hook_handles:\n",
    "    handle.remove()\n",
    "\n",
    "# Average and normalize importance scores\n",
    "combined_importance = {}\n",
    "for name in importance_scores:\n",
    "    grad_vals = np.array(importance_scores[name])\n",
    "    var_vals = np.array(output_variances[name])\n",
    "    if len(grad_vals) == 0 or len(var_vals) == 0:\n",
    "        print(f\"Warning: No valid scores for {name}, skipping\")\n",
    "        continue\n",
    "    grad_vals = np.clip(grad_vals, 1e-8, 1e2)\n",
    "    var_vals = np.clip(var_vals, 1e-8, 1e2)\n",
    "    z_grad = (grad_vals - grad_vals.mean()) / (grad_vals.std() + 1e-6)\n",
    "    z_var = (var_vals - var_vals.mean()) / (var_vals.std() + 1e-6)\n",
    "    score = 0.5 * z_grad.mean() + 0.5 * z_var.mean()\n",
    "    if not np.isnan(score):\n",
    "        combined_importance[name] = score\n",
    "\n",
    "if not combined_importance:\n",
    "    raise ValueError(\"No valid importance scores computed.\")\n",
    "\n",
    "total_importance = max(sum(abs(v) for v in combined_importance.values()), 1e-8)\n",
    "importance_norm = {k: v / total_importance for k, v in combined_importance.items()}\n",
    "\n",
    "# Proportional rank allocation with budget enforcement\n",
    "r_max, r_min, r_total = 16, 1, 100\n",
    "adaptive_ranks = {}\n",
    "total_score = sum(importance_norm.values())\n",
    "for name, score in importance_norm.items():\n",
    "    rank = max(r_min, min(r_max, int(score / total_score * r_total)))\n",
    "    adaptive_ranks[name] = rank\n",
    "\n",
    "allocated = sum(adaptive_ranks.values())\n",
    "iteration_count = 0\n",
    "while allocated != r_total:\n",
    "    iteration_count += 1\n",
    "    if allocated > r_total:\n",
    "        for name in sorted(adaptive_ranks, key=lambda x: importance_norm.get(x, 0), reverse=True):\n",
    "            if adaptive_ranks[name] > r_min:\n",
    "                adaptive_ranks[name] -= 1\n",
    "                allocated -= 1\n",
    "                if allocated == r_total:\n",
    "                    break\n",
    "    else:\n",
    "        for name in sorted(adaptive_ranks, key=lambda x: importance_norm.get(x, 0)):\n",
    "            if adaptive_ranks[name] < r_max:\n",
    "                adaptive_ranks[name] += 1\n",
    "                allocated += 1\n",
    "                if allocated == r_total:\n",
    "                    break\n",
    "print(f\"Total allocated ranks: {sum(adaptive_ranks.values())}\")\n",
    "print(f\"Rank allocation iterations: {iteration_count}\")\n",
    "\n",
    "# Custom LoRA Layer\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank, alpha=16, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.scale = alpha / rank if rank > 0 else 0\n",
    "        self.lora_A = torch.nn.Parameter(torch.randn(rank, in_features) * 0.02)\n",
    "        self.lora_B = torch.nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r = F.linear(self.dropout(x), self.lora_A)\n",
    "        r = F.linear(r, self.lora_B) * self.scale\n",
    "        return r\n",
    "\n",
    "def compute_correlation(model, val_loader, criterion, device, importance_norm):\n",
    "    model.eval()\n",
    "    baseline_loss, _, _, _, _ = validate(model, val_loader, criterion, device)\n",
    "    loss_deltas = {}\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoRAWrappedLinear) and name in importance_norm:\n",
    "            lora_layer = module.lora\n",
    "            original_A = lora_layer.lora_A.data.clone()\n",
    "            original_B = lora_layer.lora_B.data.clone()\n",
    "            lora_layer.lora_A.data.zero_()\n",
    "            lora_layer.lora_B.data.zero_()\n",
    "            ablated_loss, _, _, _, _ = validate(model, val_loader, criterion, device)\n",
    "            loss_deltas[name] = ablated_loss - baseline_loss\n",
    "            lora_layer.lora_A.data = original_A\n",
    "            lora_layer.lora_B.data = original_B\n",
    "    \n",
    "    common_keys = set(loss_deltas.keys()) & set(importance_norm.keys())\n",
    "    if len(common_keys) < 2:\n",
    "        print(\"Warning: Insufficient layers for correlation, returning 0.0\")\n",
    "        return 0.0\n",
    "    \n",
    "    importance_vals = [importance_norm[n] for n in common_keys]\n",
    "    delta_vals = [loss_deltas[n] for n in common_keys]\n",
    "    \n",
    "    if np.any(np.isnan(importance_vals)) or np.any(np.isnan(delta_vals)):\n",
    "        print(\"Warning: Invalid data for correlation, returning 0.0\")\n",
    "        return 0.0\n",
    "    \n",
    "    correlation, _ = spearmanr(importance_vals, delta_vals)\n",
    "    return correlation\n",
    "\n",
    "# Define LoRAWrappedLinear globally\n",
    "class LoRAWrappedLinear(torch.nn.Module):\n",
    "    def __init__(self, linear, lora):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = lora\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "# Inject LoRA into model\n",
    "def inject_lora(model, adaptive_ranks, lora_alpha=16, lora_dropout=0.1):\n",
    "    for name, module in list(model.named_modules()):\n",
    "        if isinstance(module, torch.nn.Linear) and name in adaptive_ranks:\n",
    "            rank = adaptive_ranks[name]\n",
    "            if rank == 0:\n",
    "                continue\n",
    "            module.weight.requires_grad = False\n",
    "            if module.bias is not None:\n",
    "                module.bias.requires_grad = False\n",
    "            lora_layer = LoRALayer(\n",
    "                in_features=module.in_features,\n",
    "                out_features=module.out_features,\n",
    "                rank=rank,\n",
    "                alpha=lora_alpha,\n",
    "                dropout=lora_dropout\n",
    "            ).to(module.weight.device)\n",
    "            wrapped_layer = LoRAWrappedLinear(module, lora_layer)\n",
    "            parent_name = name.rsplit('.', 1)[0]\n",
    "            child_name = name.rsplit('.', 1)[1]\n",
    "            parent = model\n",
    "            for part in parent_name.split('.'):\n",
    "                parent = getattr(parent, part)\n",
    "            setattr(parent, child_name, wrapped_layer)\n",
    "\n",
    "# Apply custom LoRA\n",
    "model.to(device)\n",
    "inject_lora(model, adaptive_ranks, lora_alpha=16, lora_dropout=0.1)\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameters before training:\")\n",
    "trainable_params = 0\n",
    "lora_layers = []\n",
    "for name, param in model.named_parameters():\n",
    "    if not ('lora_A' in name or 'lora_B' in name or 'classifier' in name):\n",
    "        param.requires_grad = False\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "        if 'lora_' in name:\n",
    "            lora_layers.append(name)\n",
    "print(f\"Total trainable params: {trainable_params}\")\n",
    "print(f\"LoRA layers: {lora_layers}\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "# Correlation analysis\n",
    "# correlation = compute_correlation(model, val_loader, criterion, device, importance_norm)\n",
    "# print(f\"Spearman's correlation: {correlation:.4f}\")\n",
    "\n",
    "# Fine-tune\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=1e-6)\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = \"deit_cifar10_ar_lora_baseline_cutmix_best_seed78_slowerLR.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        lr = 1e-5 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    scheduler.step(epoch + 1)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    val_loss, val_accuracy, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {best_epoch} with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    lora_norm = sum(torch.norm(param).item() for name, param in model.named_parameters() if 'lora_' in name)\n",
    "    print(f\"LoRA weight norm: {lora_norm:.4f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ar_lora_baseline_cutmix_metrics_cifar10_seed78_slowerLR.png')\n",
    "plt.close()\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "inject_lora(model, adaptive_ranks, lora_alpha=16, lora_dropout=0.1)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate test set with calibration\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "\n",
    "# Temperature scaling\n",
    "def find_optimal_temperature(val_logits, val_labels):\n",
    "    def ece_with_temp(temp):\n",
    "        scaled_probs = F.softmax(val_logits / temp, dim=1).numpy()\n",
    "        scaled_probs = np.clip(scaled_probs, 1e-5, 1-1e-5)\n",
    "        return compute_ece(scaled_probs, val_labels)\n",
    "    \n",
    "    temps = np.linspace(0.1, 5.0, 20)\n",
    "    eces = [ece_with_temp(t) for t in temps]\n",
    "    return temps[np.argmin(eces)]\n",
    "\n",
    "_, _, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "optimal_temp = find_optimal_temperature(val_logits, val_labels)\n",
    "scaled_test_probs = F.softmax(test_logits / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}) - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"deit_cifar10_ar_lora_baseline_cutmix_final_seed78_slowerLR.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb76cae-a8a2-4c40-92f4-a2a0b4ca47d3",
   "metadata": {},
   "source": [
    "# FULL FT start LR 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466a86c2-a8f2-4581-8552-fd6149206222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program starts...\n",
      "Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters before training:\n",
      "Total trainable params: 85807882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1250/1250 [03:15<00:00,  6.38it/s, loss=0.635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 1 with Val Accuracy: 95.62%\n",
      "Epoch 1, Train Loss: 0.8902, Val Loss: 0.6104, Train Accuracy: 85.06%, Val Accuracy: 95.62%, Time: 217.26s, LR: 0.000499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.34it/s, loss=0.544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.9066, Val Loss: 0.6239, Train Accuracy: 83.39%, Val Accuracy: 94.63%, Time: 218.36s, LR: 0.000495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.34it/s, loss=0.651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.9291, Val Loss: 0.6661, Train Accuracy: 83.78%, Val Accuracy: 92.67%, Time: 217.88s, LR: 0.000488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1250/1250 [03:13<00:00,  6.45it/s, loss=0.664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.9610, Val Loss: 0.7175, Train Accuracy: 81.23%, Val Accuracy: 91.01%, Time: 214.84s, LR: 0.000478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.30it/s, loss=0.94] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Test Loss: 0.6960, Test Accuracy: 91.59%\n",
      "Epoch 5, Train Loss: 1.0133, Val Loss: 0.6935, Train Accuracy: 78.81%, Val Accuracy: 91.77%, Time: 219.39s, LR: 0.000467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.34it/s, loss=0.737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.9771, Val Loss: 0.7191, Train Accuracy: 80.28%, Val Accuracy: 91.26%, Time: 218.55s, LR: 0.000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1250/1250 [03:15<00:00,  6.40it/s, loss=0.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.9449, Val Loss: 0.7458, Train Accuracy: 82.06%, Val Accuracy: 89.47%, Time: 216.24s, LR: 0.000436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1250/1250 [03:15<00:00,  6.40it/s, loss=1.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.9210, Val Loss: 0.6843, Train Accuracy: 84.17%, Val Accuracy: 92.28%, Time: 216.18s, LR: 0.000417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1250/1250 [03:15<00:00,  6.38it/s, loss=0.573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.8894, Val Loss: 0.6784, Train Accuracy: 84.44%, Val Accuracy: 92.62%, Time: 216.81s, LR: 0.000397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1250/1250 [03:16<00:00,  6.37it/s, loss=0.516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Test Loss: 0.6914, Test Accuracy: 92.70%\n",
      "Epoch 10, Train Loss: 0.8663, Val Loss: 0.6860, Train Accuracy: 85.45%, Val Accuracy: 92.93%, Time: 217.21s, LR: 0.000375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model (Epoch 1) - Test Loss: 0.6158, Test Accuracy: 95.33%\n",
      "ECE: 0.0726, Scaled ECE: 0.0215, Scaled Test Accuracy: 95.33%\n",
      "Class-wise Accuracy: Mean 0.95, Std 0.04\n",
      "Total training time: 2260.39 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from transformers import DeiTForImageClassification\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "print('Program starts...')\n",
    "print(\"Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "assert len(set(train_indices) & set(val_indices)) == 0, \"Train-validation overlap detected\"\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DeiT and modify classifier\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "\n",
    "# All parameters are trainable for full fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameters before training:\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable params: {trainable_params}\")\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), torch.cat(all_logits)\n",
    "\n",
    "# Compute ECE\n",
    "def compute_ece(probs, labels, n_bins=10):\n",
    "    probs = np.clip(probs, 1e-5, 1-1e-5)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += prop_in_bin * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "    return ece\n",
    "\n",
    "# CutMix function\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Setup training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = \"deit_cifar10_full_finetune_cutmix_best_seed78.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        lr = 5e-4 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    # Add tqdm progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{10}\", leave=True)\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    scheduler.step(epoch + 1)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    val_loss, val_accuracy, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {best_epoch} with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_finetune_cutmix_metrics_cifar10_seed78.png')\n",
    "plt.close()\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate test set with calibration\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "\n",
    "# Temperature scaling\n",
    "def find_optimal_temperature(val_logits, val_labels):\n",
    "    def ece_with_temp(temp):\n",
    "        scaled_probs = F.softmax(val_logits / temp, dim=1).numpy()\n",
    "        scaled_probs = np.clip(scaled_probs, 1e-5, 1-1e-5)\n",
    "        return compute_ece(scaled_probs, val_labels)\n",
    "    \n",
    "    temps = np.linspace(0.1, 5.0, 20)\n",
    "    eces = [ece_with_temp(t) for t in temps]\n",
    "    return temps[np.argmin(eces)]\n",
    "\n",
    "_, _, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "optimal_temp = find_optimal_temperature(val_logits, val_labels)\n",
    "scaled_test_probs = F.softmax(test_logits / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}) - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"deit_cifar10_full_finetune_cutmix_final_seed78.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39deaae-76ae-45cf-8f10-aced417e6a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8dd581-1bd1-4c94-aaeb-7eb7e02803d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7c418d5-a835-4e7d-85ea-9ae38881b2f9",
   "metadata": {},
   "source": [
    "# FULL FT start LR 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62109f3b-a7c6-4e28-a9f9-635b2c05afa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program starts...\n",
      "Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters before training:\n",
      "Total trainable params: 85807882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.32it/s, loss=0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 1 with Val Accuracy: 97.86%\n",
      "Epoch 1, Train Loss: 0.8796, Val Loss: 0.5542, Train Accuracy: 85.75%, Val Accuracy: 97.86%, Time: 218.50s, LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.31it/s, loss=0.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 2 with Val Accuracy: 97.93%\n",
      "Epoch 2, Train Loss: 0.8114, Val Loss: 0.5542, Train Accuracy: 87.19%, Val Accuracy: 97.93%, Time: 219.05s, LR: 0.000099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.34it/s, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.7837, Val Loss: 0.5625, Train Accuracy: 89.86%, Val Accuracy: 97.68%, Time: 218.43s, LR: 0.000098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.31it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.7745, Val Loss: 0.5657, Train Accuracy: 88.67%, Val Accuracy: 97.35%, Time: 218.81s, LR: 0.000096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.33it/s, loss=0.797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Test Loss: 0.5759, Test Accuracy: 97.16%\n",
      "Epoch 5, Train Loss: 0.7977, Val Loss: 0.5743, Train Accuracy: 87.64%, Val Accuracy: 97.12%, Time: 218.59s, LR: 0.000093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1250/1250 [03:16<00:00,  6.36it/s, loss=0.507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.7815, Val Loss: 0.5871, Train Accuracy: 88.20%, Val Accuracy: 96.70%, Time: 217.14s, LR: 0.000091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1250/1250 [03:14<00:00,  6.42it/s, loss=0.564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.7695, Val Loss: 0.5696, Train Accuracy: 89.35%, Val Accuracy: 97.41%, Time: 215.81s, LR: 0.000087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.31it/s, loss=0.942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.7668, Val Loss: 0.5636, Train Accuracy: 90.98%, Val Accuracy: 97.71%, Time: 218.97s, LR: 0.000084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1250/1250 [03:16<00:00,  6.36it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.7532, Val Loss: 0.5672, Train Accuracy: 89.81%, Val Accuracy: 97.46%, Time: 217.84s, LR: 0.000080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.29it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Test Loss: 0.5603, Test Accuracy: 97.85%\n",
      "Epoch 10, Train Loss: 0.7469, Val Loss: 0.5643, Train Accuracy: 90.18%, Val Accuracy: 97.71%, Time: 220.07s, LR: 0.000075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model (Epoch 2) - Test Loss: 0.5547, Test Accuracy: 97.87%\n",
      "ECE: 0.0891, Scaled ECE: 0.0051, Scaled Test Accuracy: 97.87%\n",
      "Class-wise Accuracy: Mean 0.98, Std 0.01\n",
      "Total training time: 2271.24 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from transformers import DeiTForImageClassification\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "print('Program starts...')\n",
    "print(\"Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "assert len(set(train_indices) & set(val_indices)) == 0, \"Train-validation overlap detected\"\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DeiT and modify classifier\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "\n",
    "# All parameters are trainable for full fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameters before training:\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable params: {trainable_params}\")\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), torch.cat(all_logits)\n",
    "\n",
    "# Compute ECE\n",
    "def compute_ece(probs, labels, n_bins=10):\n",
    "    probs = np.clip(probs, 1e-5, 1-1e-5)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += prop_in_bin * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "    return ece\n",
    "\n",
    "# CutMix function\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Setup training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = \"deit_cifar10_full_finetune_cutmix_best_seed78_slowLR.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        lr = 1e-4 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    # Add tqdm progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{10}\", leave=True)\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    scheduler.step(epoch + 1)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    val_loss, val_accuracy, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {best_epoch} with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_finetune_cutmix_metrics_cifar10_seed78_slowLR.png')\n",
    "plt.close()\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate test set with calibration\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "\n",
    "# Temperature scaling\n",
    "def find_optimal_temperature(val_logits, val_labels):\n",
    "    def ece_with_temp(temp):\n",
    "        scaled_probs = F.softmax(val_logits / temp, dim=1).numpy()\n",
    "        scaled_probs = np.clip(scaled_probs, 1e-5, 1-1e-5)\n",
    "        return compute_ece(scaled_probs, val_labels)\n",
    "    \n",
    "    temps = np.linspace(0.1, 5.0, 20)\n",
    "    eces = [ece_with_temp(t) for t in temps]\n",
    "    return temps[np.argmin(eces)]\n",
    "\n",
    "_, _, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "optimal_temp = find_optimal_temperature(val_logits, val_labels)\n",
    "scaled_test_probs = F.softmax(test_logits / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}) - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"deit_cifar10_full_finetune_cutmix_final_seed78_slowLR.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90233417-8c58-484c-bc43-fcbcb5c5f9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71dbd019-12f1-4baa-9969-ee5fed9fb228",
   "metadata": {},
   "source": [
    "# FULL FT start LR 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad5d259-62c9-4aad-a9e8-a42205a293f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program starts...\n",
      "Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters before training:\n",
      "Total trainable params: 85807882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1250/1250 [03:19<00:00,  6.28it/s, loss=0.585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 1 with Val Accuracy: 96.55%\n",
      "Epoch 1, Train Loss: 1.2312, Val Loss: 0.6105, Train Accuracy: 72.90%, Val Accuracy: 96.55%, Time: 220.34s, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.33it/s, loss=0.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 2 with Val Accuracy: 97.66%\n",
      "Epoch 2, Train Loss: 0.8684, Val Loss: 0.5623, Train Accuracy: 85.44%, Val Accuracy: 97.66%, Time: 218.50s, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.30it/s, loss=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 3 with Val Accuracy: 98.20%\n",
      "Epoch 3, Train Loss: 0.8018, Val Loss: 0.5479, Train Accuracy: 89.33%, Val Accuracy: 98.20%, Time: 219.25s, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.31it/s, loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 4 with Val Accuracy: 98.32%\n",
      "Epoch 4, Train Loss: 0.7647, Val Loss: 0.5413, Train Accuracy: 89.22%, Val Accuracy: 98.32%, Time: 219.18s, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.30it/s, loss=0.801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Test Loss: 0.5450, Test Accuracy: 98.20%\n",
      "New best model saved at epoch 5 with Val Accuracy: 98.49%\n",
      "Epoch 5, Train Loss: 0.7675, Val Loss: 0.5391, Train Accuracy: 88.92%, Val Accuracy: 98.49%, Time: 219.43s, LR: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1250/1250 [03:15<00:00,  6.39it/s, loss=0.502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 6 with Val Accuracy: 98.75%\n",
      "Epoch 6, Train Loss: 0.7596, Val Loss: 0.5346, Train Accuracy: 89.03%, Val Accuracy: 98.75%, Time: 216.67s, LR: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.33it/s, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.7503, Val Loss: 0.5375, Train Accuracy: 89.87%, Val Accuracy: 98.58%, Time: 218.58s, LR: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1250/1250 [03:19<00:00,  6.28it/s, loss=0.88] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.7496, Val Loss: 0.5374, Train Accuracy: 91.47%, Val Accuracy: 98.59%, Time: 220.17s, LR: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.33it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 9 with Val Accuracy: 98.76%\n",
      "Epoch 9, Train Loss: 0.7371, Val Loss: 0.5334, Train Accuracy: 90.50%, Val Accuracy: 98.76%, Time: 218.40s, LR: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.34it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Test Loss: 0.5386, Test Accuracy: 98.52%\n",
      "Epoch 10, Train Loss: 0.7330, Val Loss: 0.5360, Train Accuracy: 90.72%, Val Accuracy: 98.68%, Time: 218.17s, LR: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model (Epoch 9) - Test Loss: 0.5381, Test Accuracy: 98.51%\n",
      "ECE: 0.0840, Scaled ECE: 0.0051, Scaled Test Accuracy: 98.51%\n",
      "Class-wise Accuracy: Mean 0.99, Std 0.01\n",
      "Total training time: 2278.70 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from transformers import DeiTForImageClassification\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "print('Program starts...')\n",
    "print(\"Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "assert len(set(train_indices) & set(val_indices)) == 0, \"Train-validation overlap detected\"\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DeiT and modify classifier\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "\n",
    "# All parameters are trainable for full fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameters before training:\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable params: {trainable_params}\")\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), torch.cat(all_logits)\n",
    "\n",
    "# Compute ECE\n",
    "def compute_ece(probs, labels, n_bins=10):\n",
    "    probs = np.clip(probs, 1e-5, 1-1e-5)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += prop_in_bin * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "    return ece\n",
    "\n",
    "# CutMix function\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Setup training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = \"deit_cifar10_full_finetune_cutmix_best_seed78_slowerLR.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        lr = 1e-5 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    # Add tqdm progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{10}\", leave=True)\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    scheduler.step(epoch + 1)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    val_loss, val_accuracy, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {best_epoch} with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_finetune_cutmix_metrics_cifar10_seed78_slowerLR.png')\n",
    "plt.close()\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate test set with calibration\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "\n",
    "# Temperature scaling\n",
    "def find_optimal_temperature(val_logits, val_labels):\n",
    "    def ece_with_temp(temp):\n",
    "        scaled_probs = F.softmax(val_logits / temp, dim=1).numpy()\n",
    "        scaled_probs = np.clip(scaled_probs, 1e-5, 1-1e-5)\n",
    "        return compute_ece(scaled_probs, val_labels)\n",
    "    \n",
    "    temps = np.linspace(0.1, 5.0, 20)\n",
    "    eces = [ece_with_temp(t) for t in temps]\n",
    "    return temps[np.argmin(eces)]\n",
    "\n",
    "_, _, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "optimal_temp = find_optimal_temperature(val_logits, val_labels)\n",
    "scaled_test_probs = F.softmax(test_logits / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}) - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"deit_cifar10_full_finetune_cutmix_final_seed78_slowerLR.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f92ee-33cd-42c4-9f53-9f43ee22c303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
