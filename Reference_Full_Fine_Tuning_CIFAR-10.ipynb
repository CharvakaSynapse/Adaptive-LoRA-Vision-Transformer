{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de24f726-7c99-42ab-a696-08d6b73eba01",
   "metadata": {},
   "source": [
    "# DEIT full fine tune CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ce50be-361d-43a6-b9bc-2a32591d7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program starts...\n",
      "Running Classifier-Only Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:01<00:00, 91460144.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters before training:\n",
      "Total trainable params: 7690\n",
      "New best model saved at epoch 1 with Val Accuracy: 94.13%\n",
      "Epoch 1, Train Loss: 1.1643, Val Loss: 0.6939, Train Accuracy: 76.69%, Val Accuracy: 94.13%, Time: 100.80s, LR: 0.000499\n",
      "New best model saved at epoch 2 with Val Accuracy: 95.22%\n",
      "Epoch 2, Train Loss: 0.9845, Val Loss: 0.6637, Train Accuracy: 81.86%, Val Accuracy: 95.22%, Time: 98.87s, LR: 0.000495\n",
      "New best model saved at epoch 3 with Val Accuracy: 95.46%\n",
      "Epoch 3, Train Loss: 0.9465, Val Loss: 0.6524, Train Accuracy: 84.70%, Val Accuracy: 95.46%, Time: 99.91s, LR: 0.000488\n",
      "New best model saved at epoch 4 with Val Accuracy: 95.69%\n",
      "Epoch 4, Train Loss: 0.9258, Val Loss: 0.6497, Train Accuracy: 84.20%, Val Accuracy: 95.69%, Time: 99.26s, LR: 0.000478\n",
      "Epoch 5 - Test Loss: 0.6576, Test Accuracy: 95.24%\n",
      "Epoch 5, Train Loss: 0.9424, Val Loss: 0.6512, Train Accuracy: 83.56%, Val Accuracy: 95.61%, Time: 99.90s, LR: 0.000467\n",
      "Epoch 6, Train Loss: 0.9433, Val Loss: 0.6475, Train Accuracy: 83.50%, Val Accuracy: 95.67%, Time: 100.16s, LR: 0.000452\n",
      "New best model saved at epoch 7 with Val Accuracy: 95.88%\n",
      "Epoch 7, Train Loss: 0.9369, Val Loss: 0.6455, Train Accuracy: 84.23%, Val Accuracy: 95.88%, Time: 100.85s, LR: 0.000436\n",
      "Epoch 8, Train Loss: 0.9404, Val Loss: 0.6472, Train Accuracy: 84.91%, Val Accuracy: 95.80%, Time: 99.94s, LR: 0.000417\n",
      "Epoch 9, Train Loss: 0.9272, Val Loss: 0.6463, Train Accuracy: 84.59%, Val Accuracy: 95.74%, Time: 99.13s, LR: 0.000397\n",
      "Epoch 10 - Test Loss: 0.6525, Test Accuracy: 95.26%\n",
      "New best model saved at epoch 10 with Val Accuracy: 95.92%\n",
      "Epoch 10, Train Loss: 0.9252, Val Loss: 0.6467, Train Accuracy: 84.80%, Val Accuracy: 95.92%, Time: 99.62s, LR: 0.000375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model (Epoch 10) - Test Loss: 0.6525, Test Accuracy: 95.26%\n",
      "ECE: 0.1274, Scaled ECE: 0.0228, Scaled Test Accuracy: 95.26%\n",
      "Class-wise Accuracy: Mean 0.95, Std 0.02\n",
      "Total training time: 1088.29 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from transformers import DeiTForImageClassification\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "start = time.time()\n",
    "print('Program starts...')\n",
    "print(\"Running Classifier-Only Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "assert len(set(train_indices) & set(val_indices)) == 0, \"Train-validation overlap detected\"\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DeiT and modify classifier\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "\n",
    "# Freeze all parameters except classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameters before training:\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable params: {trainable_params}\")\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), torch.cat(all_logits)\n",
    "\n",
    "# Compute ECE\n",
    "def compute_ece(probs, labels, n_bins=10):\n",
    "    probs = np.clip(probs, 1e-5, 1-1e-5)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += prop_in_bin * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "    return ece\n",
    "\n",
    "# CutMix function\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Setup training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = \"deit_cifar10_classifier_only_cutmix_best_seed78.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        lr = 5e-4 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    scheduler.step(epoch + 1)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    val_loss, val_accuracy, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {best_epoch} with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('classifier_only_cutmix_metrics_cifar10_seed78.png')\n",
    "plt.close()\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name:\n",
    "        param.requires_grad = False\n",
    "    else:\n",
    "        param.requires_grad = True\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate test set with calibration\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "\n",
    "# Temperature scaling\n",
    "def find_optimal_temperature(val_logits, val_labels):\n",
    "    def ece_with_temp(temp):\n",
    "        scaled_probs = F.softmax(val_logits / temp, dim=1).numpy()\n",
    "        scaled_probs = np.clip(scaled_probs, 1e-5, 1-1e-5)\n",
    "        return compute_ece(scaled_probs, val_labels)\n",
    "    \n",
    "    temps = np.linspace(0.1, 5.0, 20)\n",
    "    eces = [ece_with_temp(t) for t in temps]\n",
    "    return temps[np.argmin(eces)]\n",
    "\n",
    "_, _, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "optimal_temp = find_optimal_temperature(val_logits, val_labels)\n",
    "scaled_test_probs = F.softmax(test_logits / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}) - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"deit_cifar10_classifier_only_cutmix_final_seed78.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f5c6cc-7f23-49a2-9ced-4386c3bdf0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837d597-49b6-4d2d-a571-36a44ae8823c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb76cae-a8a2-4c40-92f4-a2a0b4ca47d3",
   "metadata": {},
   "source": [
    "# FULL FT start LR 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466a86c2-a8f2-4581-8552-fd6149206222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program starts...\n",
      "Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters before training:\n",
      "Total trainable params: 85807882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1250/1250 [03:15<00:00,  6.38it/s, loss=0.635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 1 with Val Accuracy: 95.62%\n",
      "Epoch 1, Train Loss: 0.8902, Val Loss: 0.6104, Train Accuracy: 85.06%, Val Accuracy: 95.62%, Time: 217.26s, LR: 0.000499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.34it/s, loss=0.544]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.9066, Val Loss: 0.6239, Train Accuracy: 83.39%, Val Accuracy: 94.63%, Time: 218.36s, LR: 0.000495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.34it/s, loss=0.651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.9291, Val Loss: 0.6661, Train Accuracy: 83.78%, Val Accuracy: 92.67%, Time: 217.88s, LR: 0.000488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1250/1250 [03:13<00:00,  6.45it/s, loss=0.664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.9610, Val Loss: 0.7175, Train Accuracy: 81.23%, Val Accuracy: 91.01%, Time: 214.84s, LR: 0.000478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.30it/s, loss=0.94] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Test Loss: 0.6960, Test Accuracy: 91.59%\n",
      "Epoch 5, Train Loss: 1.0133, Val Loss: 0.6935, Train Accuracy: 78.81%, Val Accuracy: 91.77%, Time: 219.39s, LR: 0.000467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.34it/s, loss=0.737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.9771, Val Loss: 0.7191, Train Accuracy: 80.28%, Val Accuracy: 91.26%, Time: 218.55s, LR: 0.000452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1250/1250 [03:15<00:00,  6.40it/s, loss=0.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.9449, Val Loss: 0.7458, Train Accuracy: 82.06%, Val Accuracy: 89.47%, Time: 216.24s, LR: 0.000436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1250/1250 [03:15<00:00,  6.40it/s, loss=1.06] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.9210, Val Loss: 0.6843, Train Accuracy: 84.17%, Val Accuracy: 92.28%, Time: 216.18s, LR: 0.000417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1250/1250 [03:15<00:00,  6.38it/s, loss=0.573]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.8894, Val Loss: 0.6784, Train Accuracy: 84.44%, Val Accuracy: 92.62%, Time: 216.81s, LR: 0.000397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1250/1250 [03:16<00:00,  6.37it/s, loss=0.516]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Test Loss: 0.6914, Test Accuracy: 92.70%\n",
      "Epoch 10, Train Loss: 0.8663, Val Loss: 0.6860, Train Accuracy: 85.45%, Val Accuracy: 92.93%, Time: 217.21s, LR: 0.000375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model (Epoch 1) - Test Loss: 0.6158, Test Accuracy: 95.33%\n",
      "ECE: 0.0726, Scaled ECE: 0.0215, Scaled Test Accuracy: 95.33%\n",
      "Class-wise Accuracy: Mean 0.95, Std 0.04\n",
      "Total training time: 2260.39 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from transformers import DeiTForImageClassification\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "print('Program starts...')\n",
    "print(\"Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "assert len(set(train_indices) & set(val_indices)) == 0, \"Train-validation overlap detected\"\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DeiT and modify classifier\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "\n",
    "# All parameters are trainable for full fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameters before training:\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable params: {trainable_params}\")\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), torch.cat(all_logits)\n",
    "\n",
    "# Compute ECE\n",
    "def compute_ece(probs, labels, n_bins=10):\n",
    "    probs = np.clip(probs, 1e-5, 1-1e-5)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += prop_in_bin * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "    return ece\n",
    "\n",
    "# CutMix function\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Setup training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = \"deit_cifar10_full_finetune_cutmix_best_seed78.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        lr = 5e-4 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    # Add tqdm progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{10}\", leave=True)\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    scheduler.step(epoch + 1)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    val_loss, val_accuracy, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {best_epoch} with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_finetune_cutmix_metrics_cifar10_seed78.png')\n",
    "plt.close()\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate test set with calibration\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "\n",
    "# Temperature scaling\n",
    "def find_optimal_temperature(val_logits, val_labels):\n",
    "    def ece_with_temp(temp):\n",
    "        scaled_probs = F.softmax(val_logits / temp, dim=1).numpy()\n",
    "        scaled_probs = np.clip(scaled_probs, 1e-5, 1-1e-5)\n",
    "        return compute_ece(scaled_probs, val_labels)\n",
    "    \n",
    "    temps = np.linspace(0.1, 5.0, 20)\n",
    "    eces = [ece_with_temp(t) for t in temps]\n",
    "    return temps[np.argmin(eces)]\n",
    "\n",
    "_, _, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "optimal_temp = find_optimal_temperature(val_logits, val_labels)\n",
    "scaled_test_probs = F.softmax(test_logits / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}) - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"deit_cifar10_full_finetune_cutmix_final_seed78.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39deaae-76ae-45cf-8f10-aced417e6a2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8dd581-1bd1-4c94-aaeb-7eb7e02803d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7c418d5-a835-4e7d-85ea-9ae38881b2f9",
   "metadata": {},
   "source": [
    "# FULL FT start LR 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62109f3b-a7c6-4e28-a9f9-635b2c05afa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program starts...\n",
      "Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters before training:\n",
      "Total trainable params: 85807882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.32it/s, loss=0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 1 with Val Accuracy: 97.86%\n",
      "Epoch 1, Train Loss: 0.8796, Val Loss: 0.5542, Train Accuracy: 85.75%, Val Accuracy: 97.86%, Time: 218.50s, LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.31it/s, loss=0.533]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 2 with Val Accuracy: 97.93%\n",
      "Epoch 2, Train Loss: 0.8114, Val Loss: 0.5542, Train Accuracy: 87.19%, Val Accuracy: 97.93%, Time: 219.05s, LR: 0.000099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.34it/s, loss=0.567]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.7837, Val Loss: 0.5625, Train Accuracy: 89.86%, Val Accuracy: 97.68%, Time: 218.43s, LR: 0.000098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.31it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.7745, Val Loss: 0.5657, Train Accuracy: 88.67%, Val Accuracy: 97.35%, Time: 218.81s, LR: 0.000096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.33it/s, loss=0.797]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Test Loss: 0.5759, Test Accuracy: 97.16%\n",
      "Epoch 5, Train Loss: 0.7977, Val Loss: 0.5743, Train Accuracy: 87.64%, Val Accuracy: 97.12%, Time: 218.59s, LR: 0.000093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1250/1250 [03:16<00:00,  6.36it/s, loss=0.507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 0.7815, Val Loss: 0.5871, Train Accuracy: 88.20%, Val Accuracy: 96.70%, Time: 217.14s, LR: 0.000091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1250/1250 [03:14<00:00,  6.42it/s, loss=0.564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.7695, Val Loss: 0.5696, Train Accuracy: 89.35%, Val Accuracy: 97.41%, Time: 215.81s, LR: 0.000087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.31it/s, loss=0.942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.7668, Val Loss: 0.5636, Train Accuracy: 90.98%, Val Accuracy: 97.71%, Time: 218.97s, LR: 0.000084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1250/1250 [03:16<00:00,  6.36it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 0.7532, Val Loss: 0.5672, Train Accuracy: 89.81%, Val Accuracy: 97.46%, Time: 217.84s, LR: 0.000080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.29it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Test Loss: 0.5603, Test Accuracy: 97.85%\n",
      "Epoch 10, Train Loss: 0.7469, Val Loss: 0.5643, Train Accuracy: 90.18%, Val Accuracy: 97.71%, Time: 220.07s, LR: 0.000075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model (Epoch 2) - Test Loss: 0.5547, Test Accuracy: 97.87%\n",
      "ECE: 0.0891, Scaled ECE: 0.0051, Scaled Test Accuracy: 97.87%\n",
      "Class-wise Accuracy: Mean 0.98, Std 0.01\n",
      "Total training time: 2271.24 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from transformers import DeiTForImageClassification\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "print('Program starts...')\n",
    "print(\"Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "assert len(set(train_indices) & set(val_indices)) == 0, \"Train-validation overlap detected\"\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DeiT and modify classifier\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "\n",
    "# All parameters are trainable for full fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameters before training:\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable params: {trainable_params}\")\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), torch.cat(all_logits)\n",
    "\n",
    "# Compute ECE\n",
    "def compute_ece(probs, labels, n_bins=10):\n",
    "    probs = np.clip(probs, 1e-5, 1-1e-5)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += prop_in_bin * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "    return ece\n",
    "\n",
    "# CutMix function\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Setup training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = \"deit_cifar10_full_finetune_cutmix_best_seed78_slowLR.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        lr = 1e-4 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    # Add tqdm progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{10}\", leave=True)\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    scheduler.step(epoch + 1)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    val_loss, val_accuracy, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {best_epoch} with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_finetune_cutmix_metrics_cifar10_seed78_slowLR.png')\n",
    "plt.close()\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate test set with calibration\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "\n",
    "# Temperature scaling\n",
    "def find_optimal_temperature(val_logits, val_labels):\n",
    "    def ece_with_temp(temp):\n",
    "        scaled_probs = F.softmax(val_logits / temp, dim=1).numpy()\n",
    "        scaled_probs = np.clip(scaled_probs, 1e-5, 1-1e-5)\n",
    "        return compute_ece(scaled_probs, val_labels)\n",
    "    \n",
    "    temps = np.linspace(0.1, 5.0, 20)\n",
    "    eces = [ece_with_temp(t) for t in temps]\n",
    "    return temps[np.argmin(eces)]\n",
    "\n",
    "_, _, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "optimal_temp = find_optimal_temperature(val_logits, val_labels)\n",
    "scaled_test_probs = F.softmax(test_logits / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}) - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"deit_cifar10_full_finetune_cutmix_final_seed78_slowLR.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90233417-8c58-484c-bc43-fcbcb5c5f9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71dbd019-12f1-4baa-9969-ee5fed9fb228",
   "metadata": {},
   "source": [
    "# FULL FT start LR 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad5d259-62c9-4aad-a9e8-a42205a293f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Program starts...\n",
      "Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters before training:\n",
      "Total trainable params: 85807882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1250/1250 [03:19<00:00,  6.28it/s, loss=0.585]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 1 with Val Accuracy: 96.55%\n",
      "Epoch 1, Train Loss: 1.2312, Val Loss: 0.6105, Train Accuracy: 72.90%, Val Accuracy: 96.55%, Time: 220.34s, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.33it/s, loss=0.53] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 2 with Val Accuracy: 97.66%\n",
      "Epoch 2, Train Loss: 0.8684, Val Loss: 0.5623, Train Accuracy: 85.44%, Val Accuracy: 97.66%, Time: 218.50s, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.30it/s, loss=0.574]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 3 with Val Accuracy: 98.20%\n",
      "Epoch 3, Train Loss: 0.8018, Val Loss: 0.5479, Train Accuracy: 89.33%, Val Accuracy: 98.20%, Time: 219.25s, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.31it/s, loss=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 4 with Val Accuracy: 98.32%\n",
      "Epoch 4, Train Loss: 0.7647, Val Loss: 0.5413, Train Accuracy: 89.22%, Val Accuracy: 98.32%, Time: 219.18s, LR: 0.000010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1250/1250 [03:18<00:00,  6.30it/s, loss=0.801]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Test Loss: 0.5450, Test Accuracy: 98.20%\n",
      "New best model saved at epoch 5 with Val Accuracy: 98.49%\n",
      "Epoch 5, Train Loss: 0.7675, Val Loss: 0.5391, Train Accuracy: 88.92%, Val Accuracy: 98.49%, Time: 219.43s, LR: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1250/1250 [03:15<00:00,  6.39it/s, loss=0.502]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 6 with Val Accuracy: 98.75%\n",
      "Epoch 6, Train Loss: 0.7596, Val Loss: 0.5346, Train Accuracy: 89.03%, Val Accuracy: 98.75%, Time: 216.67s, LR: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.33it/s, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 0.7503, Val Loss: 0.5375, Train Accuracy: 89.87%, Val Accuracy: 98.58%, Time: 218.58s, LR: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1250/1250 [03:19<00:00,  6.28it/s, loss=0.88] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 0.7496, Val Loss: 0.5374, Train Accuracy: 91.47%, Val Accuracy: 98.59%, Time: 220.17s, LR: 0.000009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.33it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model saved at epoch 9 with Val Accuracy: 98.76%\n",
      "Epoch 9, Train Loss: 0.7371, Val Loss: 0.5334, Train Accuracy: 90.50%, Val Accuracy: 98.76%, Time: 218.40s, LR: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1250/1250 [03:17<00:00,  6.34it/s, loss=0.501]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Test Loss: 0.5386, Test Accuracy: 98.52%\n",
      "Epoch 10, Train Loss: 0.7330, Val Loss: 0.5360, Train Accuracy: 90.72%, Val Accuracy: 98.68%, Time: 218.17s, LR: 0.000008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DeiTForImageClassification were not initialized from the model checkpoint at facebook/deit-base-distilled-patch16-224 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model (Epoch 9) - Test Loss: 0.5381, Test Accuracy: 98.51%\n",
      "ECE: 0.0840, Scaled ECE: 0.0051, Scaled Test Accuracy: 98.51%\n",
      "Class-wise Accuracy: Mean 0.99, Std 0.01\n",
      "Total training time: 2278.70 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from transformers import DeiTForImageClassification\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "from tqdm import tqdm\n",
    "\n",
    "start = time.time()\n",
    "print('Program starts...')\n",
    "print(\"Running Full Fine-Tuning with CutMix on CIFAR-10 (15 Epochs)\")\n",
    "\n",
    "# Set seeds\n",
    "np.random.seed(78)\n",
    "torch.manual_seed(78)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Split training set into train and validation (80/20)\n",
    "dataset_size = len(train_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * dataset_size))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "assert len(set(train_indices) & set(val_indices)) == 0, \"Train-validation overlap detected\"\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_dataset = Subset(train_dataset, val_indices)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load DeiT and modify classifier\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "\n",
    "# All parameters are trainable for full fine-tuning\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Verify trainable parameters\n",
    "print(\"Trainable parameters before training:\")\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable params: {trainable_params}\")\n",
    "\n",
    "# Validation function\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_logits.append(outputs.cpu())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_val_loss = val_loss / len(loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return avg_val_loss, val_accuracy, np.concatenate(all_probs), np.concatenate(all_labels), torch.cat(all_logits)\n",
    "\n",
    "# Compute ECE\n",
    "def compute_ece(probs, labels, n_bins=10):\n",
    "    probs = np.clip(probs, 1e-5, 1-1e-5)\n",
    "    confidences = np.max(probs, axis=1)\n",
    "    predictions = np.argmax(probs, axis=1)\n",
    "    accuracies = predictions == labels\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_lowers = bin_boundaries[:-1]\n",
    "    bin_uppers = bin_boundaries[1:]\n",
    "    ece = 0.0\n",
    "    for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "        in_bin = (confidences >= bin_lower) & (confidences < bin_upper)\n",
    "        prop_in_bin = np.mean(in_bin)\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = np.mean(accuracies[in_bin])\n",
    "            avg_confidence_in_bin = np.mean(confidences[in_bin])\n",
    "            ece += prop_in_bin * np.abs(avg_confidence_in_bin - accuracy_in_bin)\n",
    "    return ece\n",
    "\n",
    "# CutMix function\n",
    "def cutmix(images, labels, alpha=1.0):\n",
    "    batch_size = images.size(0)\n",
    "    indices = torch.randperm(batch_size)\n",
    "    shuffled_images = images[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    \n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(images.size(), lam)\n",
    "    images[:, :, bby1:bby2, bbx1:bbx2] = shuffled_images[:, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.size(-1) * images.size(-2)))\n",
    "    return images, labels, shuffled_labels, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[3]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "    \n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# Setup training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=30, T_mult=1, eta_min=1e-6)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "best_val_accuracy = 0.0\n",
    "best_epoch = 1\n",
    "best_model_path = \"deit_cifar10_full_finetune_cutmix_best_seed78_slowerLR.pt\"\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    if epoch < 5:\n",
    "        lr = 1e-5 * (epoch + 1) / 5\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    # Add tqdm progress bar for training loop\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{10}\", leave=True)\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        if np.random.rand() < 0.5:\n",
    "            images, labels_a, labels_b, lam = cutmix(images, labels, alpha=1.0)\n",
    "            outputs = model(images).logits\n",
    "            loss = lam * criterion(outputs, labels_a) + (1 - lam) * criterion(outputs, labels_b)\n",
    "        else:\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        # Update progress bar with current loss\n",
    "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    progress_bar.close()\n",
    "    scheduler.step(epoch + 1)\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = 100 * train_correct / train_total\n",
    "    \n",
    "    val_loss, val_accuracy, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "    epoch_time = time.time() - start_time\n",
    "    \n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_epoch = epoch + 1\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"New best model saved at epoch {best_epoch} with Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Time: {epoch_time:.2f}s, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('full_finetune_cutmix_metrics_cifar10_seed78_slowerLR.png')\n",
    "plt.close()\n",
    "\n",
    "# Load best model for final evaluation\n",
    "model = DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "model.classifier = torch.nn.Linear(model.classifier.in_features, 10)\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.to(device)\n",
    "\n",
    "# Evaluate test set with calibration\n",
    "test_loss, test_accuracy, test_probs, test_labels, test_logits = validate(model, test_loader, criterion, device)\n",
    "ece = compute_ece(test_probs, test_labels)\n",
    "\n",
    "# Temperature scaling\n",
    "def find_optimal_temperature(val_logits, val_labels):\n",
    "    def ece_with_temp(temp):\n",
    "        scaled_probs = F.softmax(val_logits / temp, dim=1).numpy()\n",
    "        scaled_probs = np.clip(scaled_probs, 1e-5, 1-1e-5)\n",
    "        return compute_ece(scaled_probs, val_labels)\n",
    "    \n",
    "    temps = np.linspace(0.1, 5.0, 20)\n",
    "    eces = [ece_with_temp(t) for t in temps]\n",
    "    return temps[np.argmin(eces)]\n",
    "\n",
    "_, _, val_probs, val_labels, val_logits = validate(model, val_loader, criterion, device)\n",
    "optimal_temp = find_optimal_temperature(val_logits, val_labels)\n",
    "scaled_test_probs = F.softmax(test_logits / optimal_temp, dim=1).numpy()\n",
    "scaled_test_accuracy = accuracy_score(test_labels, np.argmax(scaled_test_probs, axis=1)) * 100\n",
    "scaled_ece = compute_ece(scaled_test_probs, test_labels)\n",
    "\n",
    "class_accuracies = []\n",
    "for i in range(10):\n",
    "    class_mask = test_labels == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = accuracy_score(test_labels[class_mask], np.argmax(test_probs[class_mask], axis=1))\n",
    "        class_accuracies.append(class_acc)\n",
    "class_acc_mean = np.mean(class_accuracies)\n",
    "class_acc_std = np.std(class_accuracies)\n",
    "\n",
    "print(f\"Best Model (Epoch {best_epoch}) - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"ECE: {ece:.4f}, Scaled ECE: {scaled_ece:.4f}, Scaled Test Accuracy: {scaled_test_accuracy:.2f}%\")\n",
    "print(f\"Class-wise Accuracy: Mean {class_acc_mean:.2f}, Std {class_acc_std:.2f}\")\n",
    "print(f\"Total training time: {(time.time() - start):.2f} seconds\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"deit_cifar10_full_finetune_cutmix_final_seed78_slowerLR.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167f92ee-33cd-42c4-9f53-9f43ee22c303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
